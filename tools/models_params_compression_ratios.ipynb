{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32f422c",
   "metadata": {},
   "source": [
    "# CODE THAT CAOUNTS PARAMETERS OF A MODEL AND EXAMPLARY COMPRESSION RATIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00032f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 8,030,261,248\n",
      "LLaMA-3 param count: 1,498,482,688\n",
      "LLaMA-3 param count: 833,942,016\n",
      "LLaMA-3 param count: 304,286,720\n",
      "LLaMA-3 param count: 409,007,040\n",
      "LLaMA-3 param count: 1,812,039,680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8030261248, 1498482688, 833942016, 304286720, 409007040, 1812039680)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def llama3_param_count_v2(\n",
    "    n_layers: int,\n",
    "    d_model: int,\n",
    "    n_heads: int,\n",
    "    n_kv_heads: int,\n",
    "    vocab_size: int,\n",
    "    ffn_dim: int,\n",
    "    datt:int,\n",
    "    gated:bool = True\n",
    "    # untied_lm_head: bool = False,  # if True, count a separate LM head (vocab*d_model)\n",
    "    # include_bias: bool = True,     # whether to count biases in linear layers\n",
    "):\n",
    "    \"\"\"\n",
    "    More accurate parameter counter for LLaMA-3 style models:\n",
    "      - Uses explicit ffn_dim\n",
    "      - Supports GQA (grouped KV heads)\n",
    "      - Counts optional biases\n",
    "      - Handles tied or untied embeddings\n",
    "    \"\"\"\n",
    "    # === Embeddings ===\n",
    "    token_embed = vocab_size * d_model\n",
    "    # lm_head = vocab_size * d_model if untied_lm_head else 0\n",
    "    lm_head = vocab_size * d_model\n",
    "\n",
    "    # === Per-layer ===\n",
    "    # Q projection: d_model -> d_model\n",
    "    q_w = d_model * datt\n",
    "    # q_b = d_model if include_bias else 0\n",
    "\n",
    "    # K/V projections: d_model -> kv_out_dim\n",
    "    kv_out_dim = int(round(datt * (n_kv_heads / n_heads)))\n",
    "    k_w = d_model * kv_out_dim\n",
    "    v_w = d_model * kv_out_dim\n",
    "    # k_b = kv_out_dim if include_bias else 0\n",
    "    # v_b = kv_out_dim if include_bias else 0\n",
    "\n",
    "    # Output projection: d_model -> d_model\n",
    "    out_w = d_model * d_model\n",
    "    # out_b = d_model if include_bias else 0\n",
    "\n",
    "    # SwiGLU FFN\n",
    "    w1 = d_model * ffn_dim\n",
    "    wgate = d_model * ffn_dim\n",
    "    w2 = ffn_dim * d_model\n",
    "    # b_w1 = ffn_dim if include_bias else 0\n",
    "    # b_wgate = ffn_dim if include_bias else 0\n",
    "    # b_w2 = d_model if include_bias else 0\n",
    "    if gated:\n",
    "        ffn_total = w1 + wgate + w2\n",
    "    else:\n",
    "        ffn_total = w1 + w2\n",
    "    # ffn_bias = b_w1 + b_wgate + b_w2\n",
    "\n",
    "    # Norms (2 per layer + final)\n",
    "    rms_per_layer = 2 * d_model\n",
    "    rms_final = d_model\n",
    "\n",
    "    # per_layer = q_w + q_b + k_w + k_b + v_w + v_b + out_w + out_b + ffn_total + ffn_bias + rms_per_layer\n",
    "    per_layer = q_w + k_w + v_w + out_w + ffn_total + rms_per_layer\n",
    "    total = token_embed + n_layers * per_layer + rms_final + lm_head\n",
    "    return total\n",
    "\n",
    "\n",
    "def from_llama_8B(\n",
    "                d_model=4096,\n",
    "                dff=14336,\n",
    "                n_layers=32,\n",
    "                datt=4096,\n",
    "                n_heads=32,\n",
    "                n_kv_heads=8,\n",
    "                vocab_size=128256):\n",
    "    params = llama3_param_count_v2(n_layers=n_layers,d_model=d_model,n_heads=n_heads,n_kv_heads=n_kv_heads,vocab_size=vocab_size,ffn_dim=dff, datt=datt)\n",
    "    print(f\"LLaMA-3 param count: {params:,}\")\n",
    "    return params\n",
    "\n",
    "def from_llama_1B(\n",
    "                d_model=2048,\n",
    "                dff=8192,\n",
    "                n_layers=16,\n",
    "                datt=2048,\n",
    "                n_heads=32,\n",
    "                n_kv_heads=8,\n",
    "                vocab_size=128256):\n",
    "    params = llama3_param_count_v2(n_layers=n_layers,d_model=d_model,n_heads=n_heads,n_kv_heads=n_kv_heads,vocab_size=vocab_size,ffn_dim=dff, datt=datt)\n",
    "    print(f\"LLaMA-3 param count: {params:,}\")\n",
    "    return params\n",
    "\n",
    "def from_llmr_800(\n",
    "                d_model=1536,\n",
    "                dff=6144,\n",
    "                n_layers=24,\n",
    "                datt=1536,\n",
    "                n_heads=24,\n",
    "                n_kv_heads=24,\n",
    "                vocab_size=50257):\n",
    "    params = llama3_param_count_v2(n_layers=n_layers,d_model=d_model,n_heads=n_heads,n_kv_heads=n_kv_heads,vocab_size=vocab_size,ffn_dim=dff, datt=datt, gated=False)\n",
    "    print(f\"LLaMA-3 param count: {params:,}\")\n",
    "    return params\n",
    "\n",
    "def from_llmr_300(\n",
    "                d_model=1024,\n",
    "                dff=4096,\n",
    "                n_layers=16,\n",
    "                datt=1024,\n",
    "                n_heads=16,\n",
    "                n_kv_heads=16,\n",
    "                vocab_size=50257):\n",
    "    params = llama3_param_count_v2(n_layers=n_layers,d_model=d_model,n_heads=n_heads,n_kv_heads=n_kv_heads,vocab_size=vocab_size,ffn_dim=dff, datt=datt, gated=False)\n",
    "    print(f\"LLaMA-3 param count: {params:,}\")\n",
    "    return params\n",
    "\n",
    "def from_llmr_360(\n",
    "                d_model=960,\n",
    "                dff=2560,\n",
    "                n_layers=32,\n",
    "                datt=960,\n",
    "                n_heads=15,\n",
    "                n_kv_heads=5,\n",
    "                vocab_size=49152):\n",
    "    params = llama3_param_count_v2(n_layers=n_layers,d_model=d_model,n_heads=n_heads,n_kv_heads=n_kv_heads,vocab_size=vocab_size,ffn_dim=dff, datt=datt, gated=True)\n",
    "    print(f\"LLaMA-3 param count: {params:,}\")\n",
    "    return params\n",
    "\n",
    "def from_sllm_1700(\n",
    "                d_model=2048,\n",
    "                dff=8192,\n",
    "                n_layers=24,\n",
    "                datt=2048,\n",
    "                n_heads=32,\n",
    "                n_kv_heads=32,\n",
    "                vocab_size=49152):\n",
    "    params = llama3_param_count_v2(n_layers=n_layers,d_model=d_model,n_heads=n_heads,n_kv_heads=n_kv_heads,vocab_size=vocab_size,ffn_dim=dff, datt=datt, gated=True)\n",
    "    print(f\"LLaMA-3 param count: {params:,}\")\n",
    "    return params\n",
    "\n",
    "from_llama_8B(), from_llama_1B(), from_llmr_800(), from_llmr_300(), from_llmr_360(), from_sllm_1700()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ae1a7",
   "metadata": {},
   "source": [
    "# RATIOS FOR PAPER MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d958f",
   "metadata": {},
   "source": [
    "### LLAMA3 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e329c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 786,574,656.0\n",
      "LLaMA-3 param count: 1,498,482,688\n",
      "llama3 1B 50% compression ratio 0.4750859237153896\n",
      "dmodel: 1344.0, dff: 5376.0\n",
      "\n",
      "LLaMA-3 param count: 485,161,920.0\n",
      "LLaMA-3 param count: 1,498,482,688\n",
      "llama3 1B 70% compression ratio 0.6762312144910145\n",
      "dmodel: 960.0, dff: 3840.0\n",
      "\n",
      "LLaMA-3 param count: 178,698,688.0\n",
      "LLaMA-3 param count: 1,498,482,688\n",
      "llama3 1B 90% compression ratio 0.8807469119055982\n",
      "dmodel: 448.0, dff: 1792.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Configs \n",
    "\n",
    "# dmodel: 1344 # 50\n",
    "# dff: 5376\n",
    "\n",
    "# dmodel: 960 # 30\n",
    "# dff: 3840\n",
    "\n",
    "# dmodel: 448 # 10\n",
    "# dff: 1728\n",
    "\n",
    "bdmodel = 2048 # llama3 1B\n",
    "bdff = 8192 # llama3 1B\n",
    "\n",
    "# 50% 0.525\n",
    "ratio = 1/64*42\n",
    "\n",
    "print(f\"llama3 1B 50% compression ratio {1 - from_llama_1B(2048*ratio, 8192*ratio)/from_llama_1B()}\")\n",
    "print(f\"dmodel: {bdmodel*ratio}, dff: {bdff*ratio}\\n\")\n",
    "\n",
    "# 30% 0.324\n",
    "ratio = 1/64*30\n",
    "\n",
    "print(f\"llama3 1B 70% compression ratio {1 - from_llama_1B(2048*ratio, 8192*ratio)/from_llama_1B()}\")\n",
    "print(f\"dmodel: {bdmodel*ratio}, dff: {bdff*ratio}\\n\")\n",
    "\n",
    "# 10% 0.119\n",
    "ratio = 1/64*14\n",
    "\n",
    "print(f\"llama3 1B 90% compression ratio {1 - from_llama_1B(2048*ratio, 8192*ratio)/from_llama_1B()}\")\n",
    "print(f\"dmodel: {bdmodel*ratio}, dff: {bdff*ratio}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78248680",
   "metadata": {},
   "source": [
    "### LLAMA3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a731c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 4,412,083,200\n",
      "LLaMA-3 param count: 8,030,261,248\n",
      "Minitrin compression ratio: 0.45056791258206397\n",
      "Minitron compression ratios, dmodel ratio: 0.75, dff ratio: 0.6428571428571429\n",
      "dmodel: 3072.0, dff: 9216.0\n",
      "\n",
      "LLaMA-3 param count: 4,412,083,200.0\n",
      "LLaMA-3 param count: 8,030,261,248\n",
      "llama3 8B 50% compression ratio: 0.45056791258206397\n",
      "dmodel: 3072.0, dff: 9216.0\n",
      "\n",
      "LLaMA-3 param count: 2,471,871,600.0\n",
      "LLaMA-3 param count: 8,030,261,248\n",
      "llama3 8B 70% compression ratio: 0.6921804255601722\n",
      "dmodel: 2160.0, dff: 6480.0\n",
      "\n",
      "LLaMA-3 param count: 835,406,880.0\n",
      "LLaMA-3 param count: 8,030,261,248\n",
      "llama3 8B 90% compression ratio: 0.8959676585605401\n",
      "dmodel: 1056.0, dff: 3168.0000000000005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bdmodel = 4096 # llama3 8B\n",
    "bdff = 14336 # llama3 8B\n",
    "\n",
    "# 3072/4096, 9216/14336\n",
    "\n",
    "m_dmodel_r = 3072/bdmodel\n",
    "m_bdff_r = 9216/bdff\n",
    "print(f\"Minitrin compression ratio: {1-from_llama_8B(3072, 9216)/from_llama_8B()}\")\n",
    "print(f\"Minitron compression ratios, dmodel ratio: {m_dmodel_r}, dff ratio: {m_bdff_r}\")\n",
    "print(f\"dmodel: {bdmodel*m_dmodel_r}, dff: {bdff*m_bdff_r}\\n\")\n",
    "\n",
    "\n",
    "# 50%\n",
    "ratio = 1/64*64\n",
    "dmodel_r = m_dmodel_r * ratio\n",
    "dff_r = m_bdff_r * ratio\n",
    "print(f\"llama3 8B 50% compression ratio: {1 - from_llama_8B(bdmodel*dmodel_r, bdff*dff_r)/from_llama_8B()}\")\n",
    "print(f\"dmodel: {bdmodel*dmodel_r}, dff: {bdff*dff_r}\\n\")\n",
    "\n",
    "# 70% # (2240, 7840) # 0.3564\n",
    "ratio = 1/64*45\n",
    "dmodel_r = m_dmodel_r * ratio\n",
    "dff_r = m_bdff_r * ratio\n",
    "print(f\"llama3 8B 70% compression ratio: {1 - from_llama_8B(bdmodel*dmodel_r, bdff*dff_r)/from_llama_8B()}\")\n",
    "print(f\"dmodel: {bdmodel*dmodel_r}, dff: {bdff*dff_r}\\n\")\n",
    "\n",
    "\n",
    "# 90% # (1024, 3584) # 0.1058\n",
    "ratio = 1/64*22\n",
    "dmodel_r = m_dmodel_r * ratio\n",
    "dff_r = m_bdff_r * ratio\n",
    "print(f\"llama3 8B 90% compression ratio: {1 - from_llama_8B(bdmodel*dmodel_r, bdff*dff_r)/from_llama_8B()}\")\n",
    "print(f\"dmodel: {bdmodel*dmodel_r}, dff: {bdff*dff_r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b305b",
   "metadata": {},
   "source": [
    "# OTHER MODELS RATIO - NOT IN PAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f4a8d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 4,034,214,592.0\n",
      "LLaMA-3 param count: 8,030,261,248\n",
      "llama3 1B 50% compression ratio 0.49762349350654655\n",
      "dmodel: 2752.0, dff: 9632.0\n",
      "\n",
      "LLaMA-3 param count: 2,471,626,752.0\n",
      "LLaMA-3 param count: 8,030,261,248\n",
      "llama3 1B 70% compression ratio 0.6922109162244781\n",
      "dmodel: 2048.0, dff: 7168.0\n",
      "\n",
      "LLaMA-3 param count: 849,937,408.0\n",
      "LLaMA-3 param count: 8,030,261,248\n",
      "llama3 1B 90% compression ratio 0.8941581871683585\n",
      "dmodel: 1024.0, dff: 3584.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conjoinded ratio cdmodel = cdff !!! - DONT USE IN PAPER EXPS - USE MINITRON RATIO ABOVE\n",
    "bdmodel = 4096 # llama3 8B\n",
    "bdff = 14336 # llama3 8B\n",
    "\n",
    "# 50%\n",
    "ratio = 1/64*43\n",
    "print(f\"llama3 1B 50% compression ratio {1 - from_llama_8B(bdmodel*ratio, bdff*ratio)/from_llama_8B()}\")\n",
    "print(f\"dmodel: {bdmodel*ratio}, dff: {bdff*ratio}\\n\")\n",
    "\n",
    "# 70% # (2240, 7840) # 0.3564\n",
    "ratio = 1/64*32\n",
    "print(f\"llama3 1B 70% compression ratio {1 - from_llama_8B(bdmodel*ratio, bdff*ratio)/from_llama_8B()}\")\n",
    "print(f\"dmodel: {bdmodel*ratio}, dff: {bdff*ratio}\\n\")\n",
    "\n",
    "\n",
    "# 90% # (1024, 3584) # 0.1058\n",
    "ratio = 1/64*16\n",
    "print(f\"llama3 1B 90% compression ratio {1 - from_llama_8B(bdmodel*ratio, bdff*ratio)/from_llama_8B()}\")\n",
    "print(f\"dmodel: {bdmodel*ratio}, dff: {bdff*ratio}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef310791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 80,720,970.0\n",
      "LLaMA-3 param count: 409,007,040\n",
      "0.1973583877676042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(330.0, 880.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdmodel = 960\n",
    "bdff = 2560\n",
    "\n",
    "\n",
    "# 50%\n",
    "ratio = 1/32*21\n",
    "# 35%\n",
    "ratio = 1/32*16\n",
    "# 20%\n",
    "ratio = 1/32*11\n",
    "\n",
    "print(from_llmr_360(bdmodel*ratio, bdff*ratio)/from_llmr_360())\n",
    "bdmodel*ratio, bdff*ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5257ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 171,707,184.0\n",
      "LLaMA-3 param count: 833,942,016\n",
      "0.20589822878045277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(528.0, 2112.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdmodel = 1536\n",
    "bdff = 6144\n",
    "\n",
    "\n",
    "# 10%\n",
    "ratio = 1/32*11\n",
    "print(from_llmr_800(bdmodel*ratio, bdff*ratio)/from_llmr_800())\n",
    "bdmodel*ratio, bdff*ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc8ba50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 105,033,120.0\n",
      "LLaMA-3 param count: 304,286,720\n",
      "0.34517812673520554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(480.0, 1920.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdmodel = 1024\n",
    "bdff = 4096\n",
    "\n",
    "# 10%\n",
    "ratio = 1/32*15\n",
    "print(from_llmr_300(bdmodel*ratio, bdff*ratio)/from_llmr_300())\n",
    "bdmodel*ratio, bdff*ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b1fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 928,966,496.0\n",
      "LLaMA-3 param count: 1,812,039,680\n",
      "0.512663440129523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1376.0, 5504.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdmodel = 2048\n",
    "bdff = 8192\n",
    "\n",
    "\n",
    "# 50% ~0.524\n",
    "ratio = 1/64*43\n",
    "\n",
    "# 30% ~0.324\n",
    "# ratio = 1/64*32\n",
    "\n",
    "# 10% ~0.119\n",
    "# ratio = 1/64*16\n",
    "\n",
    "print(from_sllm_1700(bdmodel*ratio, bdff*ratio)/from_sllm_1700())\n",
    "bdmodel*ratio, bdff*ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f15ce079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-3 param count: 849,937,408.0\n",
      "LLaMA-3 param count: 8,030,261,248\n",
      "0.10584181283164151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1024.0, 3584.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50%\n",
    "ratio = 1/32*8\n",
    "\n",
    "ratio_ff = ratio #1/64*41\n",
    "print(from_llama_8B(4096*ratio, 14336*ratio_ff)/from_llama_8B())\n",
    "\n",
    "4096*ratio, 14336*ratio_ff\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanollmr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
