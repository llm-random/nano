{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3605fdbf",
   "metadata": {},
   "source": [
    "### Devstral 2\n",
    "assumptions:\n",
    "- dense\n",
    "- params: 128B\n",
    "- 17B tokens in 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443035c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 136\n",
    "dmodel = 64 * n_layers\n",
    "n_experts = 1\n",
    "dff = n_experts * 4 * (dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "931d6c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found k, so that active ~ 123B.\n",
      "k: 136\n",
      "dmodel: 8704\n",
      "active / total: 124.07B / 124.51B\n",
      "active / total: 124074.89M / 124510.09M\n"
     ]
    }
   ],
   "source": [
    "attn_params = 4 * (dmodel ** 2)\n",
    "ff_params = 2 * dmodel * dff\n",
    "ff_active = 8 * (dmodel ** 2)\n",
    "embedding = 50_000 * dmodel\n",
    "model_params = n_layers * (attn_params + ff_params) + 2 * embedding\n",
    "model_active = n_layers * (attn_params + ff_active) + embedding\n",
    "\n",
    "print(f\"found k, so that active ~ 123B.\\nk: {n_layers}\\ndmodel: {dmodel}\")\n",
    "\n",
    "print(f\"active / total: {model_active / 1e9:.2f}B / {model_params / 1e9:.2f}B\")\n",
    "print(f\"active / total: {model_active / 1e6:.2f}M / {model_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ba60a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think it's before long context extension\n",
      "tokens: 17.83B\n"
     ]
    }
   ],
   "source": [
    "print(\"I think it's before long context extension\")\n",
    "\n",
    "sl = 4096\n",
    "bs = 256\n",
    "steps = 17_000\n",
    "tokens = bs * sl * steps\n",
    "print(f\"tokens: {tokens / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a895248",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 24 * 60 ** 2\n",
    "h100 = (1_979 / 2) * 1e12\n",
    "mfu = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "add1464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralAI GPUs uded for Devstral 2 estimate: 388\n",
      "Helios has 440 * H100 for comparison\n",
      "Early stage uses batch size warmup, which I'm not accounting for\n"
     ]
    }
   ],
   "source": [
    "th_flops = model_active * tokens * 6\n",
    "r_flops_per_gpu = time * h100 * mfu\n",
    "n_gpus = th_flops / r_flops_per_gpu\n",
    "\n",
    "print(f\"MistralAI GPUs uded for Devstral 2 estimate: {n_gpus:.0f}\")\n",
    "print(\"Helios has 440 * H100 for comparison\")\n",
    "print(\"Early stage uses batch size warmup, which I'm not accounting for\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
