# @package _global_
defaults:
  - /_cluster/entropy@_here_
  - /_model/tiny@_here_
  - /_trainer/llama@_here_
  - /_dataset/c4@_here_
  - /_checkpoints/none@_here_
  - /_misc/default@_here_
  - /_eval/none@_here_
  - _self_

common:
  sequence_length: 2048
  batch_size: 32
  dmodel: 1024
  dff: 2724
  datt: ${common.dmodel}
  n_blocks: 16
  q_heads: 16
  kv_heads: 16
  vocab_size: 128256

trainer:
  _target_: src.product_keys.trainer.MaskedLMTrainer
  masking_percentage: 0.2
  mask_token_id: 50257
  unmaskable_special_tokens: [50256, 50257]  # <|endoftext|>
  gradient_accumulation_steps: 2
  n_steps: 101410
  learning_rate: 5e-4
  train_dataloader:
    tokenize_fn:
      _target_: src.product_keys.datasets.gpt2_mask_tokenize_fn
      

  eval_dataloader:
    tokenize_fn:
      _target_: src.product_keys.datasets.gpt2_mask_tokenize_fn


infrastructure:
  metric_logger:
    name: masked_language_modeling
    project_name: pmtest/tml-bgw
    tags:
      - nano
      - masked_language_modeling
      - "seq_len=${common.sequence_length}"
      - "n_layers=${common.n_blocks}"
  slurm:
    gres: gpu:1
    time: "1-00:00:00"
    job-name: ${infrastructure.metric_logger.name}

model:
  encoder:
    block_fn:
      attention_fn:
        _target_: src.product_keys.model.RoPETopKAttention
        _partial_: true
        dmodel: ${common.dmodel}
        q_heads: ${common.q_heads}
        kv_heads: ${common.kv_heads}
        seq_len: ${common.sequence_length}

        q_proj_fn:
          _target_: src.projected_compression.model.Linear
          _partial_: true
          in_features: ${common.dmodel}
          out_features: ${common.datt}
          partial_init_fn:
            _target_: src.projected_compression.model.llm_random_weight_init
            _partial_: true
            scale: 1

        k_proj_fn:
          _target_: src.projected_compression.model.Linear
          _partial_: true
          in_features: ${common.dmodel}
          out_features: ${eval:'(${common.datt} // ${model.encoder.block_fn.attention_fn.q_heads}) * ${model.encoder.block_fn.attention_fn.kv_heads}'}
          partial_init_fn:
            _target_: src.projected_compression.model.llm_random_weight_init
            _partial_: true
            scale: 1

        v_proj_fn: ${model.encoder.block_fn.attention_fn.k_proj_fn}

        o_proj_fn: 
          _target_: src.projected_compression.model.Linear
          _partial_: true
          in_features: ${common.datt}
          out_features: ${common.dmodel}
          partial_init_fn:
            _target_: src.projected_compression.model.llm_random_weight_init
            _partial_: true
            scale: 1

        rope_base: 500000
        rope_scale_freqs: true
        top_k: 600000
        top_k_before_softmax: true
