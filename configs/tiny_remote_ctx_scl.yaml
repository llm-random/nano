defaults:
  - _cluster@_here_: entropy
  - _model@_here_: tiny
  - _trainer@_here_: context_scaling
  - _dataset@_here_: c4
  - _checkpoints@_here_: none
  - _misc@_here_: default
  - _eval@_here_: none
  - _self_

common:
  sequence_length: 512
  batch_size: 32

trainer:
  gradient_accumulation_steps: 1
  n_steps: 101
  learning_rate: 2e-3
  eval_interval: 20
  eval_long_ctx_interval: 20

  eval_long_ctx_dataloader:
    _target_: src.core.datasets.get_dataloader
    dataset:
      _target_: src.context_scaling.WholeDocumentDataset
      base_dataset:
        _target_: src.core.datasets.C4Dataset
        sequence_length: ${common.sequence_length}
        path: /storage_nvme_1/llm-random/datasets/c4/long_context_2048n8192
        split: validation
        seed: 123
        use_new_sampling_method: true
        shuffle: true
        world_size_independent: false
        tokenize_fn: 
          _target_: src.core.datasets.gpt2_tokenize_fn
    num_workers: 8
    total_batch_size: ${common.batch_size}

infrastructure:
  metric_logger:
    name: tiny_remote
    tags:
      - nano
      - remote
      - tiny
      - context_scaling
      - "1712"

  slurm:
    time: "00:10:00"
    gres: gpu:1
    job-name: ${infrastructure.metric_logger.name}