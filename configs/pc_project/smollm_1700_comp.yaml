defaults:
  - ../_cluster@_here_: entropy 
  - ../_model/smollm@_here_: projected_1700
  - ../_trainer@_here_: llama
  - ../_dataset@_here_: fineweb
  - ../_checkpoints@_here_: none
  - ../_misc@_here_: default


common:
  sequence_length: 1024
  batch_size: 512
  
  # dmodel: 448 # 10
  # dff: 1728
  # dmodel: 960 # 30
  # dff: 3840
  # dmodel: 1344 # 50
  # dff: 5376
  # dmodel: 1728 # 75
  # dff: 6912
  dmodel: 1088 # 35
  dff: 4352
  # dmodel: 768 # 20
  # dff: 3072


trainer:
  gradient_accumulation_steps: 16
  ^n_steps: 
    - 2048 # 1B tokens
    # - 4096
    # - 6144
    # - 8192
    # - 12288
    # - 16384
    
  # learning_rate: ??? # CHANGE # optimal 10%
  # learning_rate: ??? # CHANGE # optimal 20%
  learning_rate: 13 # optimal 35%
  # learning_rate: ??? # CHANGE # optimal 1B 50%
  # learning_rate: ??? # CHANGE # optimal 75%
  # learning_rate: ??? # CHANGE #optimal fo randomi 50%

  scheduler:
    warmup_steps: 80

  checkpoint:
   load:
    type: huggingface
    path: "HuggingFaceTB/SmolLM-1.7B"
   save:
    type: nano
    path: ??? # CHANGE

  train_dataloader:
    tokenize_fn: 
      _target_: src.core.datasets.smollm_1700_tokenize_fn

  eval_dataloader:
    tokenize_fn: 
      _target_: src.core.datasets.smollm_1700_tokenize_fn


infrastructure:
  metric_logger:
    name: SLLM_1700_comp
    tags:
      - nano
      - pc
      - smollm_1700
      - projected_compression
      # - 10p
      # - 20p
      # - 30p
      - 35p
      # - 50p

  slurm: 
    job-name: ${infrastructure.metric_logger.name}
    gres: gpu:4
    time: "0-12:00:00"


apply_functions:
  - _target_: src.projected_compression.compression.init_compression
    _partial_: true
    dimensions_importances_path: ??? # CHANGE
    target_dmodel: ${common.dmodel}
    target_dff: ${common.dff}