defaults:
  - ../_cluster@_here_: entropy
  - ../_model@_here_: llama_3_1
  - ../_trainer@_here_: llama
  - ../_dataset@_here_: c4 # not used - loads slower
  - ../_checkpoints@_here_: none
  - ../_misc@_here_: default


common: 
  batch_size: 512

  # dmodel: 448 # 10
  # dff: 1728
  dmodel: 960 # 30
  dff: 3840
  # dmodel: 1344 # 50
  # dff: 5376
  # dmodel: 1728 # 75
  # dff: 6912


trainer:
  n_steps: 0 # not used
  learning_rate: 1 # not used
  gradient_accumulation_steps: 1 # not used

  checkpoint:
   load:
    # type: finalized_pc
    type: nano
    path: ??? # CHANGE
   save:
    type: huggingface
    path: ??? # CHANGE
  

infrastructure:
  metric_logger:
    name: conversion
    tags:
      - nano
      - pc
      - llama_1
      - hf_conversion
      # - TA10B
      # - randominit
      - projected_compression
      # - hard_pruning
      # - 10p
      # - 30p
      # - 50p

  slurm: 
    gres: gpu:4
    time: "0-00:30:00"