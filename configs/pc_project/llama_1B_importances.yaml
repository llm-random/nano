defaults:
  - ../_cluster@_here_: entropy
  - ../_model@_here_: llama_3_1
  - ../_trainer@_here_: pc_llama
  - ../_dataset@_here_: fineweb
  - ../_checkpoints@_here_: none
  - ../_misc@_here_: default


common:
  sequence_length: 1024
  batch_size: 4


trainer:
  gradient_accumulation_steps: 8 # not used ?
  n_steps: 0 # not used
  learning_rate: 1 # not used

  checkpoint:
   load:
    type: huggingface
    path: "meta-llama/Llama-3.2-1B"

   save:
    type: nano
    path: ??? # CHANGE


infrastructure:
  metric_logger:
    name: Llame_importances
    tags:
      - nano
      - pc
      - llama_1
      - IMPORTANCES
      - fineweb

  slurm:
    job-name: importances_llama_1
    gres: gpu:1
    nodes: 1
    time: "0-2:00:00"


apply_functions:
  - _target_: src.projected_compression.weight_importances.dummy_importances
    _partial_: true
    dmodel: ${common.dmodel}
    dff: ${common.dff}
    n_blocks: ${model.encoder.n_blocks}
    checkpoint_save_path: ${trainer.checkpoint.save.path}
  - _target_: src.projected_compression.weight_importances.minitron_importances
    _partial_: true
    dataloader: ${trainer.train_dataloader}
    dmodel: ${common.dmodel}
    dff: ${common.dff}
    calibration_dataset_size: 8192 # nvidia used 2k steps of 4k sequence lenght - this is the saturaion poin - longer doesnt improve miningfully
    seq_len: ${common.sequence_length}
    total_batch_size: ${trainer.train_dataloader.total_batch_size}
    n_blocks: ${model.encoder.n_blocks}
    checkpoint_save_path: ${trainer.checkpoint.save.path}
  - _target_: src.projected_compression.weight_importances.magnitude_importances
    _partial_: true
    dmodel: ${common.dmodel}
    dff: ${common.dff}
    n_blocks: ${model.encoder.n_blocks}
    checkpoint_save_path: ${trainer.checkpoint.save.path}
  - _target_: src.core.utils.finish_exp
    _partial_: true