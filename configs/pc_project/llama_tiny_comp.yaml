defaults:
  - ../_cluster@_here_: entropy
  - ../_model@_here_: llama_tiny_compressor
  - ../_trainer@_here_: llama
  - ../_dataset@_here_: fineweb
  - ../_checkpoints@_here_: none
  - ../_misc@_here_: default
  

common:
  sequence_length: 1024
  batch_size: 128


trainer:
  gradient_accumulation_steps: 1
  n_steps: 300
  
  learning_rate: ??? # CHANGE


infrastructure:
  metric_logger:
    name: Llame_PC
    tags:
      - nano
      - pc
      - tiny_llama
      - projected_compression

  

