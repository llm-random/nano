defaults:
  - ../_cluster@_here_: entropy
  - ../_model/llama@_here_: projected_mem_eff_1B
  - ../_trainer@_here_: llama3_1B_distillation
  - ../_dataset@_here_: fineweb
  - ../_checkpoints@_here_: none
  - ../_misc@_here_: default

common:
  sequence_length: 1024
  batch_size: 512

  dmodel: 448 # 10
  dff: 1728
  # dmodel: 960 # 30
  # dff: 3840
  # dmodel: 1344 # 50
  # dff: 5376

trainer:
  _target_: src.projected_compression.trainer_distillation.PCDistillationTrainer
  gradient_accumulation_steps: 8
  n_steps: 2048
  only_compress_model_gradient_clipping: true
  eval_interval: 100000000 # Important - Eval doesnt work with distillation!
  
  learning_rate: 13 # optimal for llama 1B 1BT

  scheduler:
    warmup_steps: 40

  distributed:
    fsdp2: null

projected_compression:
  modules_to_shard:
    - src.projected_compression.mem_eff.CompressibleBlock
    - ${model.source_model.embedding._target_}
    - ${model.source_model.encoder.block_fn._target_}
    - ${model.source_model.head._target_}

  source_model_path: ??? # CHANGE
  path_to_importances: ??? # CHANGE

  init_norms_with_ones: false
  separate_block_optimizers: true
  adjust_grad_norm: false

distillation:
  load:
    type: pc_memeff_base

init_model_opt_sched_fn: 
  source_model_for_distillation: true

model:
  cast_bfloat16: true # whether to cast bfloat16 when calculating compressed matrix

infrastructure:
  metric_logger:
    name: Llame_PC
    tags:
      - nano
      - pc
      - llama_1
      - projected_compression
      - 10p
      - mem_eff
      - distillation

  slurm: 
    job-name: ${infrastructure.metric_logger.name}
    gres: gpu:4
    time: "0-24:00:00"