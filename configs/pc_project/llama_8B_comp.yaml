defaults:
  - ../_cluster@_here_: entropy
  - ../_model/llama@_here_: projected_8B
  - ../_trainer@_here_: llama
  - ../_dataset@_here_: fineweb
  - ../_checkpoints@_here_: none
  - ../_misc@_here_: default
  

common:
  batch_size: 512


trainer:
  gradient_accumulation_steps: 4

  n_steps: 1001
  
  learning_rate: ??? # CHANGE

  checkpoint:
    save:
      type: nano
      path: null
    load:
      type: huggingface
      path: "meta-llama/Llama-3.1-8B"
      model_checkpoint_filename: __model_checkpoint_filename.pt
      training_state_filename: null
      only_weights: true 


infrastructure:
  metric_logger:
    name: Llame_PC
    tags:
      - nano
      - pc
      - llama_8
      - projected_compression

  slurm:
    time: "0-01:00:00"