defaults:
  - ../_cluster@_here_: entropy
  - ../_model/llmrandom@_here_: projected_300
  - ../_trainer@_here_: llama
  - ../_dataset@_here_: c4
  - ../_checkpoints@_here_: none
  - ../_misc@_here_: default


common:
  sequence_length: 512
  batch_size: 512
  
  dmodel: 480 # 35
  dff: 1920
  # dmodel: 320 # 20
  # dff: 1280
  # dmodel: ads # 65
  # dff: asd


trainer:
  gradient_accumulation_steps: 1
  ^n_steps: 
    - 4001 # 1B tokens
    # - 8001
    # - 16001
    # - 24001

  learning_rate: ??? # CHANGE

  scheduler:
    warmup_steps: 80
  
  checkpoint:
   load:
    type: llm-random
    path: ??? # CHANGE
   save:
    type: nano
    path: ??? # CHANGE

  train_dataloader:
    tokenize_fn: 
      _target_: src.core.datasets.get_tokenize_fn
      model_name: gpt2

  eval_dataloader:
    tokenize_fn: 
      _target_: src.core.datasets.get_tokenize_fn
      model_name: gpt2


infrastructure:
  metric_logger:
    name: LLMR_300_PC
    tags:
      - nano
      - pc
      - llmr_300
      - projected_compression
      # - 65p
      - 35p
      # - 20p

  slurm: 
    job-name: ${infrastructure.metric_logger.name}
    gres: gpu:4
    time: "0-6:00:00"


apply_functions:
  - _target_: src.projected_compression.compression.init_compression
    _partial_: true
    dimensions_importances_path: ??? # CHANGE
    target_dmodel: ${common.dmodel}
    target_dff: ${common.dff}