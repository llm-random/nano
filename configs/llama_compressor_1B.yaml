
defaults:
  - _misc@_here_: default
  - _cluster@_here_: helios
  - _model/llama@_here_: projected_1B
  - _trainer@_here_: llama
  - _dataset@_here_: c4


common:
  sequence_length: 1024
  batch_size: 512


trainer:
  gradient_accumulation_steps: 8
  n_steps: 2048

  learning_rate: 6.103515625e-05

  checkpoint:
   load:
    type: huggingface
    path: "meta-llama/Llama-3.2-1B"

infrastructure:
  metric_logger:
    name: Llame_PC
    tags:
      - nano
      - pc
      - dummy_importances
      - model=llama_3_2_1B

  slurm: 
    gres: gpu:4

apply_functions:
  - _target_: src.projected_compression.compression.init_compression
    _partial_: true
    checkpoint_load_path: ${trainer.checkpoint.save.path}
    dimensions_importances_path: "llama3_2_1B/random_dimensions_importances.pt"
    target_dmodel: ${common.dmodel}
    target_dff: ${common.dff}
