defaults:
  - _cluster@_here_: entropy
  - _model/llama@_here_: projected_1B
  - _trainer@_here_: llama
  - _dataset@_here_: c4
  - _checkpoints@_here_: none
  - _misc@_here_: default
  

common:
  sequence_length: 1024
  batch_size: 512
  dff: 5376 # 50%
  dmodel: 1344


trainer:
  gradient_accumulation_steps: 8
  n_steps: 0
  
  learning_rate: 6.103515625e-05

  checkpoint:
   save:
    type: pc_finalize
    path: null #change
   load: #change
    type: huggingface
    path: "meta-llama/Llama-3.2-1B" 


infrastructure:
  metric_logger:
    name: Llame_PC
    tags:
      - nano
      - pc
      - llama_8_pc_finalized

  slurm: 
    gres: gpu:4
