defaults:
  - _misc@_here_: default
  - _cluster@_here_: local
  - _model/llama@_here_: base_pc_model
  - _trainer@_here_: llama
  - _dataset@_here_: local_dummy
  # - ~trainer/train_dataloader/dataset
  # - helper

# common:
#   batch_size: 4
#   base_dmodel: 8
#   base_dff: 32
#   dmodel: 6
#   dff: 24
#   dhead: 4
#   n_blocks: 2
#   sequence_length: 2048
#   q_heads: 2
#   kv_heads: 2

common:
  batch_size: 32
  dmodel: 1344 # 50% parameter compression
  dff: 5376 # 50% parameter compression
  dhead: 64
  sequence_length: 1024
  base_dmodel: 2048
  base_dff: 8192
  n_blocks: 16
  q_heads: 32
  kv_heads: 8


# model:
#   embedding:
#     embedding:
#         num_embeddings: 16

trainer:
  gradient_accumulation_steps: 1
  n_steps: 4

  learning_rate: 13

  scheduler:
    warmup_steps: 2

  checkpoint:
   load:
    type: huggingface
    path: "meta-llama/Llama-3.2-1B"

  # train_dataloader:
  #   dataset:
  #     _target_: src.core.datasets.DummyDataset
  #     sequence_length: ${common.sequence_length}
  #     high: ${model.embedding.embedding.num_embeddings}
  #     seed: 1000


  # eval_dataloader:
  #   dataset:
  #     _target_: src.core.datasets.DummyDataset
  #     sequence_length: ${common.sequence_length}
  #     high: ${model.embedding.embedding.num_embeddings}
  #     seed: 123

apply_functions:
  - _target_: src.projected_compression.compression.init_compression
    _partial_: true
    dimensions_importances_path: "/net/scratch/hscra/plgrid/plgcrewtool/importances/1b/random_dimensions_importances.pt"
    target_dmodel: ${common.dmodel}
    target_dff: ${common.dff}