
common:
  _target_: src.definitions.Common
  dff: 8192
  dmodel: 2048
  datt: 2048
  q_heads: 32
  kv_heads: 8
  n_blocks: 16
  vocab_size: 128256
  sequence_length: 512
  head_norm: true
  init_scale: 0.1
  init_type: truncated_normal_fixed
  model_type: gpt
  
model:
  _target_: src.core.model.LLM

  embedding:
    _target_: src.core.model.TokenEmbedding
    vocab_size: ${common.vocab_size}
    embedding_dim: ${common.dmodel}
    init_type: ${common.init_type}
    init_scale: ${common.init_scale} #/storage_nvme_3/mstefaniak/llama_8_half_compression

  tower:
    _target_: src.core.model.TransformerTower
    n_blocks: ${common.n_blocks}
    block_fn:
      _target_: src.core.model.TransformerBlock
      _partial_: true
      residual_fn:
        _target_: src.core.model.PreNormBlock
        _partial_: true
        dmodel: ${common.dmodel}
        norm_class:
          _target_: src.core.model.RMSNorm
          _partial_: true
          eps: 1e-5

      attention_fn:
        _target_: src.core.llama.LlamaAttention
        _partial_: true
        dmodel: ${common.dmodel}
        datt: ${common.datt}
        q_heads: ${common.q_heads}
        kv_heads: ${common.kv_heads}
        seq_len: ${common.sequence_length}
        causal: true
        init_scale: ${common.init_scale}
        init_type: ${common.init_type}

      ff_layer_fn:
        _target_: src.core.llama.LLamaFeedForward
        _partial_: true
        dmodel: ${common.dmodel}
        dff: ${common.dff}
        init_scale: ${common.init_scale}
        init_type: ${common.init_type}

  head:
    _target_: src.core.model.PredictionHead
    embedding_dim: ${common.dmodel}
    output_size: ${common.vocab_size}
    init_type: ${common.init_type}
    init_scale: ${common.init_scale}
    use_layer_norm: ${common.head_norm}
