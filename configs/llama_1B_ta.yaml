defaults:
  # - _cluster@_here_: helios
  - _cluster@_here_: entropy
  # - _cluster@_here_: entropy_a100
  - _model@_here_: llama_3_1
  - _trainer@_here_: llama
  - _dataset@_here_: fineweb
  - _checkpoints@_here_: none
  - _misc@_here_: default


common:
  sequence_length: 1024
  batch_size: 512


trainer:
  gradient_accumulation_steps: 8
  ^n_steps: 
    # - 2048
    # - 4096
    # - 6144
    # - 8192
    - 20480
    # - 40960
    # - 61440


  ^exp_learning_rate: 
    # - 14
    - 15
    # - 16

  checkpoint:
   load:
    type: huggingface
    path: "meta-llama/Llama-3.2-1B"
   save:
    type: nano
    path: null #change


infrastructure:
  metric_logger:
    name: Llame_TA
    tags:
      - nano
      - pc
      - llama_1
      - TA

  slurm:
    job-name: ${infrastructure.metric_logger.name}
    gres: gpu:4
    time: "2-00:00:00"