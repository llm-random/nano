common:
  _target_: src.definitions.Common
  dff: 192
  dmodel: 48
  head_norm: true
  init_scale: 0.1
  init_type: truncated_normal_fixed
  model_type: gpt
  sequence_length: 32
  vocab_size: 50257
  base_dmodel: 64
  base_dff: 256
  
model:
  _target_: src.projected_compression.model.LLM

  embedding:
    _target_: src.projected_compression.model.ProjectedEmbedding
    embedding_fn:
      _target_: src.projected_compression.model.TransformerEmbedding
      _partial_: true
      vocab_size: ${common.vocab_size}
      dmodel: ${common.base_dmodel}
      init_fn:
        _target_: src.projected_compression.model.trunc_normal_
        _partial_: true

    result_out_features: ${common.dmodel} 
    

  encoder:
    _target_: src.projected_compression.model.TransformerEncoder
    n_blocks: 4
    block_fn:
      _target_: src.projected_compression.model.TransformerBlock
      _partial_: true
      norm_fn: 
        _target_: src.core.model.LayerNorm
        _partial_: true
        eps: 1e-5
        normalized_shape: ${common.base_dmodel}
        bias: false
      
      attention_fn:
        _target_: src.projected_compression.model.ProjectedLlamaAttention
        _partial_: true
        dmodel: ${common.base_dmodel}
        q_heads: 4
        kv_heads: 4
        seq_len: ${common.sequence_length}
        q_proj_fn:
          _target_: src.projected_compression.model.ProjectedLinear
          _partial_: true
          result_in_features: ${common.dmodel} 
          result_out_features: null
          base_in_features: ${common.base_dmodel}
          base_out_features: ${common.base_dmodel}

        k_proj_fn:
          _target_: src.projected_compression.model.ProjectedLinear
          _partial_: true
          result_in_features: ${common.dmodel} 
          result_out_features: null
          base_in_features: ${common.base_dmodel}
          base_out_features: ${eval:'${model.encoder.block_fn.attention_fn.q_heads} // ${model.encoder.block_fn.attention_fn.kv_heads} * ${common.base_dmodel}'}

        v_proj_fn: ${model.encoder.block_fn.attention_fn.k_proj_fn}

        o_proj_fn:
          _target_: src.projected_compression.model.ProjectedLinear
          _partial_: true
          result_in_features: null
          result_out_features: ${common.dmodel} 
          base_in_features: ${common.base_dmodel}
          base_out_features: ${common.base_dmodel}
        
        rope_base: 10000
        rope_scale_freqs: false



      ff_layer_fn:
        _target_: src.projected_compression.model.FeedForward
        _partial_: true
        ff_pre_act_fn:
          _target_: src.projected_compression.model.ProjectedLinear
          _partial_: true
          result_in_features: ${common.dmodel} 
          result_out_features: ${common.dff}  
          base_in_features: ${common.base_dmodel}
          base_out_features: ${common.base_dff}
        ff_post_act_fn: 
          _target_: src.projected_compression.model.ProjectedLinear
          _partial_: true
          result_in_features: ${common.dff}
          result_out_features: ${common.dmodel} 
          base_in_features: ${common.base_dff}
          base_out_features: ${common.base_dmodel}


  head:
    _target_: src.projected_compression.model.TransformerHead
    linear_fn:
      _target_: src.projected_compression.model.ProjectedLinear
      _partial_: true
      result_in_features: ${common.dmodel}
      result_out_features: null
      base_in_features: ${common.base_dmodel}
      base_out_features: ${common.vocab_size}
    norm_fn: 
        _target_: src.core.model.LayerNorm
        _partial_: true
        eps: 1e-5
        normalized_shape: ${common.base_dmodel}
        bias: false

trainer:
  _partial_: true
  _target_: src.core.trainer.Trainer

  checkpoint:
    save:
      interval: 12500
      model_checkpoint_filename: __model_optim_scheduler__.pt
      training_state_filename: __training_state__.pt
      path: checkpoints # Change me!

    load:
      type: "llm-random"
      path: "path_to_ckpt" # Change me!

  eval_interval: 100
  n_eval_steps: 5

  gradient_accumulation_steps: 1
  gradient_clipping: 0.5 # 1.0
  n_steps: 100

  learning_rate: 0.001
  scheduler:
    _partial_: true
    _target_: src.core.schedulers.get_cosine_scheduler_with_warmup
    final_lr_fraction: 0.1
    warmup_steps: 500

  weight_decay: 0.1

  train_dataloader:
    _target_: src.core.datasets.get_dataloader
    dataset_path: data
    dataset_split: train
    dataset_type: c4
    num_workers: 0 # Change me!
    seed: 20001
    sequence_length: 32
    shuffle: false
    total_batch_size: 32
    use_new_sampling_method: true
    world_size_independent: false

  eval_dataloader:
    _target_: src.core.datasets.get_dataloader
    dataset_path: data_eval
    dataset_split: validation
    dataset_type: c4
    num_workers: 0 # Change me!
    seed: 123
    sequence_length: 32
    shuffle: true
    total_batch_size: 32
    use_new_sampling_method: true
    world_size_independent: false

  distributed: null
    # fsdp:
    #   mixed_precision:
    #     dtype: bfloat16
    #     ignored_classes:
    #       - LayerNorm
    #       - AttentionMechanism
    #   modules_to_wrap:
    #     - TransformerBlock
    #     - EmbeddingLayer
    #     - PredictionHead

apply_functions:
  - _target_: src.projected_compression.compression.init_compression
    _partial_: true
    dmodel: ${common.dmodel}
    dff: ${common.dff}

infrastructure:
  slurm:
    cpus-per-task: 32
    gres: gpu:2
    job-name: test
    mem_per_gpu: 125G
    nodes: 1
    partition: h100
    time: "0-1:00:00"
    ntasks-per-node: 2

  venv_path: $HOME/nano/.venv/bin/activate # Change me!
  experiment_prepare_venv_path: ${infrastructure.venv_path}

  cemetery_experiments_dir: ~/llmrandom_cemetery

  metric_logger:
    _target_: src.definitions.MetricLoggerConfig
    heavy_metrics_calculation_interval: 100
    name: test
    # new_neptune_job: false
    project_name: pmtest/llm-random
    tags:
      - test
    # type: neptune
    type: stdout

  git:
    remote_name: cemetery
    remote_url: git@github.com:llm-random/llm-random-cemetery.git

  server: entropia

  generated_configs_path: generated_configs
