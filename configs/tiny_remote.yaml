defaults:
  - _cluster@_here_: entropy
  - _model@_here_: tiny
  - _trainer@_here_: llama
  - _dataset@_here_: c4
  - _checkpoints@_here_: none
  - _misc@_here_: default
  - _eval@_here_: default

common:
  sequence_length: 128
  batch_size: 16

trainer:
  gradient_accumulation_steps: 1
  n_steps: 100
  learning_rate: 1e-3

  # checkpoint:
  #   save:
  #     type: huggingface
  #     path: checkpoint

infrastructure:
  max_concurrent_jobs: 1

  metric_logger:
    name: tiny_remote
    tags:
      - nano
      - remote
      - tiny

  slurm:
    time: "00:10:00"
    gres: gpu:1
    job-name: ${infrastructure.metric_logger.name}

# evaluator:
#   limit: null