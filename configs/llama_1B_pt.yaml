defaults:
  # - _cluster@_here_: helios_4
  - _cluster@_here_: entropy_4
  - _model@_here_: llama_3_1
  - _trainer@_here_: llama
  - _dataset@_here_: c4
  - _checkpoints@_here_: none
  - _misc@_here_: default
  - _mas@_here_: crewtool
  # - _mas@_here_: helios


common:
  sequence_length: 1024
  batch_size: 512

  # 50%
  dmodel: 1344
  dff: 5376


trainer:
  eval_interval: 500
  gradient_accumulation_steps: 4
  # n_steps: 2048
  # n_steps: 1001
  ^n_steps: 
   - 4096
   - 6144
# [0.0078125, 0.00390625, 0.001953125, 0.0009765625, 0.00048828125, 0.000244140625, 0.0001220703125, 6.103515625e-05
  learning_rate: 0.0009765625
  # ^learning_rate: 
  #   - 0.0009765625
  #   - 0.001953125
  #   - 0.00048828125


  checkpoint:
   save:  
    path: null

  train_dataloader:
    seed: 1

  eval_dataloader:
    seed: 124

  

infrastructure:
  metric_logger:
    name: Llame_PC
    tags:
      - nano
      - pc
      - llama_1
      - pretraining

  server: entropy_c

