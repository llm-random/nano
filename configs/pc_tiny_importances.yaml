defaults:
  - _cluster@_here_: entropy
  - _model/llama@_here_: tiny
  - _trainer@_here_: llama
  - _dataset@_here_: c4
  - _checkpoints@_here_: none
  - _misc@_here_: default

common:
  sequence_length: 32
  batch_size: 4

trainer:
  n_steps: 0

  learning_rate: 3e-5

  checkpoint:
   load:
    type: huggingface
    path: "pc-project/tiny-llama-16M"
   save:
    type: nano
    path: ??? # change

infrastructure:
  metric_logger:
    name: ${infrastructure.slurm.job-name}
    tags:
      - nano
      - pc
      - IMPORTANCES

  slurm:
    job-name: importances
    gres: gpu:1
    nodes: 1
    time: "0-1:00:00"


apply_functions_after_load:
  - _target_: src.projected_compression.weight_importances.dummy_importances
    _partial_: true
    dmodel: ${common.dmodel}
    dff: ${common.dff}
    n_blocks: ${model.encoder.n_blocks}
    checkpoint_save_path: ${trainer.checkpoint.save.path}

  - _target_: src.projected_compression.weight_importances.minitron_importances
    _partial_: true
    dataloader: ${trainer.train_dataloader}
    dmodel: ${common.dmodel}
    dff: ${common.dff}
    calibration_dataset_size: 32
    seq_len: ${common.sequence_length}
    total_batch_size: ${trainer.train_dataloader.total_batch_size}
    n_blocks: ${model.encoder.n_blocks}
    checkpoint_save_path: ${trainer.checkpoint.save.path}
