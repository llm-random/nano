
trainer:
  _partial_: true
  _target_: src.core.trainer.Trainer
  eval_interval: 100
  n_eval_steps: 10
  gradient_accumulation_steps: 1
  gradient_clipping: 1.0
  n_steps: 0 # TODO change to ??? when missing config will be fixed with ^n_steps
  exp_learning_rate: null
  learning_rate: null
  weight_decay: 0.1

  scheduler:
    _partial_: true
    _target_: src.core.schedulers.get_cosine_scheduler_with_warmup
    final_lr_fraction: 0.1
    warmup_steps: ${eval:'int(${trainer.n_steps} * 0.01)'}

  distributed:
    fsdp2:
      modules_to_shard: 
        - ${model.embedding._target_}
        - ${model.encoder.block_fn._target_}
        - ${model.head._target_}
  
  train_dataloader:
    tokenize_fn:
      _target_: src.core.datasets.llama_tokenize_fn
      

  eval_dataloader:
    tokenize_fn:
      _target_: src.core.datasets.llama_tokenize_fn
