# working 22.08

defaults:
  - _cluster@_here_: helios_4
  # - _cluster@_here_: entropy_4
  - _model@_here_: llama_3_1
  - _trainer@_here_: llama
  - _dataset@_here_: c4
  - _checkpoints@_here_: none
  - _misc@_here_: default
  - _mas@_here_: helios

common: 
  batch_size: 512

trainer:
  n_steps: 101
  # n_steps: 0
  learning_rate: 3.0517578125e-05
  gradient_accumulation_steps: 8

  scheduler:
    _partial_: true
    _target_: src.core.schedulers.get_cosine_scheduler_with_warmup
    final_lr_fraction: 0.01
    warmup_steps: 40

  checkpoint:
    save:
      path: null #change me
    load:
      path: null #change me

  train_dataloader:
    num_workers: 12
    seed: 20001
    total_batch_size: ${common.batch_size}

  eval_dataloader:
    num_workers: 12
    seed: 20001
    total_batch_size: ${common.batch_size}

infrastructure:
  metric_logger:
    name: test
    tags:
      - pc
      - nano
      - ft_llama3_1
      - helios_ft

