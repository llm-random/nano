defaults:
  - _cluster@_here_: helios
  - _model@_here_: k16
  - _trainer@_here_: context_scaling
  - _dataset@_here_: ctx_scl_dataset
  - _checkpoints@_here_: none
  - _misc@_here_: default
  - _eval@_here_: none
  - _self_

common:
  sequence_length: 2048
  batch_size: 128
  ^kv_heads: [16, 1]

trainer:
  gradient_accumulation_steps: 1
  n_steps: 40_000
  ^learning_rate: [9, 10, 11, 12, 13]
  eval_interval: 100
  eval_long_ctx_interval: 1000
  n_eval_long_ctx_steps: 64

  train_dataloader:
    num_workers: 8
    dataset:
      load_dataset_fn:
        path: ${cluster_switch.train_path_c4}

  eval_dataloader:
    num_workers: 8
    dataset:
      load_dataset_fn:
        path: ${cluster_switch.eval_path_c4}

  eval_long_ctx_dataloader:
    num_workers: 8
    dataset:
      load_dataset_fn:
        # path: /storage_nvme_1/llm-random/datasets/c4/long_context_2048n8192
        path: /net/scratch/hscra/plgrid/plgj321m/c4/long_context_2048n8192

infrastructure:
  metric_logger:
    name: context_scaling
    tags:
      - nano
      - k16
      - context_scaling
      - 2copt

  slurm:
    time: "16:00:00"
    gres: gpu:4
    job-name: ${infrastructure.metric_logger.name}

    # python src/context_scaling/long_context_batch_maker.py --dataset_path /net/scratch/hscra/plgrid/plgmaciejpioro/c4/validation --save_data_dir //net/scratch/hscra/plgrid/plgmaciejpioro/c4/long_context --tokenizer gpt2 --min_length 3072 --save_batch_size 4096 --save_hist_dir tmp --num_workers 16

# rsync -rlphvP /storage_nvme_1/llm-random/datasets/c4/long_context_2048n8192 plgj321m@helios.cyfronet.pl:/net/scratch/hscra/plgrid/plgmaciejpioro/c4/long_context_2048n8192